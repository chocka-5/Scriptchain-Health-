{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Issues using positional encoding when we design a deep architecture to represent a sequence by stacking self-attention layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although positional encoding is a commonly employed technique to incorporate sequence information in self-attention layers, it has several drawbacks when utilized in deep architectures.\n",
    "\n",
    "1) Fixed Positional Embedding: When deep architectures consist of multiple self-attention layers, they may struggle to effectively model long-range dependencies. This is due to the fixed nature of positional encoding, which fails to dynamically adapt to the context and thus struggles to capture complex dependencies and patterns.\n",
    "\n",
    "2) Increased Computational Complexity: Each position in the sequence requires unique embeddings, resulting in additional computational complexity. In deep architectures with multiple self-attention layers, this added complexity can significantly increase the computational cost of both training and inference processes.\n",
    "\n",
    "3) Lack of Context Information: Positional encoding treats each position in the sequence independently, failing to explicitly capture the context or relationships between positions. Consequently, the model's ability to learn long-range dependencies is limited.\n",
    "\n",
    "4) Reliance on prior knowledge of sequence length: Positional encoding assumes that the model possesses prior knowledge of the sequence length during training. This assumption leads to the usage of padding or truncating sentences during inference, which may not generalize well to real-world problems.\n",
    "\n",
    "5) Redundant Information: In deep architectures employing multiple self-attention layers with fixed positional encoding, the same positional encoding is redundantly used across all layers. This redundancy limits the learning capacity of the model, whereas learnable positional encodings can enable the network to capture more intricate information in deeper layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Design a learnable positional encoding method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_df = pd.read_csv('C:/Users/chock/Desktop/ScriptML/amazon_cells_labelled.txt',names=['sentence', 'label'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>The screen does get smudged easily because it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>What a piece of junk.. I lose more calls on th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Item Does Not Match Picture.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The only thing that disappoint me is the infra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>You can not answer calls with the unit, never ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  label\n",
       "995  The screen does get smudged easily because it ...      0\n",
       "996  What a piece of junk.. I lose more calls on th...      0\n",
       "997                       Item Does Not Match Picture.      0\n",
       "998  The only thing that disappoint me is the infra...      0\n",
       "999  You can not answer calls with the unit, never ...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = amazon_df['sentence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, embedding_dim)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(self.embedding_dim)\n",
    "        x = x + self.pe[:x.size(0), :].squeeze()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [sentence.split() for sentence in sentences]\n",
    "vocab = set([word for sentence in tokenized_sentences for word in sentence])\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
    "\n",
    "# Define the maximum sentence length\n",
    "max_seq_length = max([len(sentence) for sentence in tokenized_sentences])\n",
    "\n",
    "indexed_sentences = []\n",
    "for sentence in tokenized_sentences:\n",
    "    indexed_sentence = [word_to_idx[word] for word in sentence]\n",
    "    indexed_sentence += [0] * (max_seq_length - len(sentence))  # Padding\n",
    "    indexed_sentences.append(indexed_sentence)\n",
    "\n",
    "# Convert indexed sentences to PyTorch tensors\n",
    "indexed_sentences = torch.tensor(indexed_sentences)\n",
    "\n",
    "# Define word embedding dimension\n",
    "embedding_dim = 100\n",
    "\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Create the positional encoding layer\n",
    "pos_encoding = PositionalEncoding(embedding_dim, max_seq_length)\n",
    "\n",
    "# Apply word embeddings and positional encoding\n",
    "embedded_sentences = embedding(indexed_sentences)\n",
    "embedded_sentences = pos_encoding(embedded_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_encoding(index_no):\n",
    "    print(sentences[index_no])\n",
    "    print(\"\\n\")\n",
    "    print(embedded_sentences[index_no].shape)\n",
    "    print(embedded_sentences[index_no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So there is no way for me to plug it in here in the US unless I go by a converter.\n",
      "\n",
      "\n",
      "torch.Size([30, 100])\n",
      "tensor([[ -9.2940, -15.5578,   2.2448,  ...,   1.2913,  -2.7797,   5.7666],\n",
      "        [  7.4501,  24.4456, -10.6322,  ...,   9.5298,   2.2946,   5.5580],\n",
      "        [-14.5885,   6.7318,  -4.3790,  ...,   6.5755,  15.6758,  14.7288],\n",
      "        ...,\n",
      "        [ -3.2380,   8.4697,  -4.1641,  ...,   8.0339,  -3.3613,  -1.4214],\n",
      "        [ -3.9234,   7.7992,  -4.6775,  ...,   8.0339,  -3.3612,  -1.4214],\n",
      "        [ -4.8580,   8.0138,  -4.5621,  ...,   8.0339,  -3.3611,  -1.4214]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print_encoding(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
